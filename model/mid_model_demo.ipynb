{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misinformation detection model (MID) full structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer, ViTFeatureExtractor, ViTModel, ViTForImageClassification\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seed(seed):\n",
    "    '''\n",
    "    Fixes random number generator seeds for reproducibility\n",
    "    '''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "same_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.path.expanduser('~')\n",
    "TEXT_DATADIR = '~/Projects/Datasets/public_news_set'\n",
    "IMAGE_DATADIR = 'Projects/Datasets/public_image_set'\n",
    "# TRAIN_FILE = \"train_1000.tsv\"\n",
    "# TEST_FILE = \"test_100.tsv\"\n",
    "# VALID_FLIE = \"valid_100.tsv\"\n",
    "TRAIN_FILE = \"new_train_with_sentiment.tsv\"\n",
    "TEST_FILE = \"new_test_with_sentiment.tsv\"\n",
    "VALID_FLIE = \"new_valid_with_sentiment.tsv\"\n",
    "\n",
    "SUFFIX = '.jpg'\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# TEXT_MODEL_CKPT = \"distilbert-base-uncased\"\n",
    "# TEXT_MODEL_CKPT = \"bert-base-uncased\"\n",
    "TEXT_MODEL_CKPT = \"roberta-base\"\n",
    "IMAGE_MODEL_CKPT = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "MAX_LENGTH = 32\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_train = pd.read_csv(os.path.join(TEXT_DATADIR, TRAIN_FILE), sep='\\t')\n",
    "mid_test = pd.read_csv(os.path.join(TEXT_DATADIR, TEST_FILE), sep='\\t')\n",
    "mid_val = pd.read_csv(os.path.join(TEXT_DATADIR, VALID_FLIE), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>domain</th>\n",
       "      <th>hasImage</th>\n",
       "      <th>id</th>\n",
       "      <th>image_url</th>\n",
       "      <th>linked_submission_id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>2_way_label</th>\n",
       "      <th>3_way_label</th>\n",
       "      <th>6_way_label</th>\n",
       "      <th>pos</th>\n",
       "      <th>neu</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jnoble50</td>\n",
       "      <td>red skull</td>\n",
       "      <td>1.553267e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>ej4e1lj</td>\n",
       "      <td>https://i.imgur.com/eD7QGRM.jpg</td>\n",
       "      <td>b44rhx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>psbattle_artwork</td>\n",
       "      <td>Red Skull</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gtash</td>\n",
       "      <td>cafe in bangkok with the cutest employees ever...</td>\n",
       "      <td>1.559911e+09</td>\n",
       "      <td>nynno.com</td>\n",
       "      <td>True</td>\n",
       "      <td>bxu2dd</td>\n",
       "      <td>https://external-preview.redd.it/MS7vkNibB3Yq1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34</td>\n",
       "      <td>upliftingnews</td>\n",
       "      <td>Cafe in Bangkok With the Cutest Employees Ever...</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202655</td>\n",
       "      <td>0.357452</td>\n",
       "      <td>0.439893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RoyalPrinceSoldier</td>\n",
       "      <td>he betrayed him</td>\n",
       "      <td>1.400820e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>chp14h4</td>\n",
       "      <td>http://i.imgur.com/9Q9CCDn.jpg</td>\n",
       "      <td>269qyi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>psbattle_artwork</td>\n",
       "      <td>He betrayed him!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>penguinseed</td>\n",
       "      <td>alderman wants to know exactly what bong shops...</td>\n",
       "      <td>1.403114e+09</td>\n",
       "      <td>dnainfo.com</td>\n",
       "      <td>True</td>\n",
       "      <td>28h8p1</td>\n",
       "      <td>https://external-preview.redd.it/lwbRUIzyGF5sU...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>nottheonion</td>\n",
       "      <td>Alderman Wants to Know Exactly What 'Bong Shop...</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202655</td>\n",
       "      <td>0.357452</td>\n",
       "      <td>0.439893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DM90</td>\n",
       "      <td>man accused of stalking scots police officer s...</td>\n",
       "      <td>1.383750e+09</td>\n",
       "      <td>dailyrecord.co.uk</td>\n",
       "      <td>True</td>\n",
       "      <td>1q10us</td>\n",
       "      <td>https://external-preview.redd.it/_fNXvGtKcKn_U...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23</td>\n",
       "      <td>nottheonion</td>\n",
       "      <td>Man accused of stalking Scots police officer s...</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202655</td>\n",
       "      <td>0.357452</td>\n",
       "      <td>0.439893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                        clean_title  \\\n",
       "0            jnoble50                                          red skull   \n",
       "1               Gtash  cafe in bangkok with the cutest employees ever...   \n",
       "2  RoyalPrinceSoldier                                    he betrayed him   \n",
       "3         penguinseed  alderman wants to know exactly what bong shops...   \n",
       "4                DM90  man accused of stalking scots police officer s...   \n",
       "\n",
       "    created_utc             domain  hasImage       id  \\\n",
       "0  1.553267e+09                NaN      True  ej4e1lj   \n",
       "1  1.559911e+09          nynno.com      True   bxu2dd   \n",
       "2  1.400820e+09                NaN      True  chp14h4   \n",
       "3  1.403114e+09        dnainfo.com      True   28h8p1   \n",
       "4  1.383750e+09  dailyrecord.co.uk      True   1q10us   \n",
       "\n",
       "                                           image_url linked_submission_id  \\\n",
       "0                    https://i.imgur.com/eD7QGRM.jpg               b44rhx   \n",
       "1  https://external-preview.redd.it/MS7vkNibB3Yq1...                  NaN   \n",
       "2                     http://i.imgur.com/9Q9CCDn.jpg               269qyi   \n",
       "3  https://external-preview.redd.it/lwbRUIzyGF5sU...                  NaN   \n",
       "4  https://external-preview.redd.it/_fNXvGtKcKn_U...                  NaN   \n",
       "\n",
       "   num_comments  score         subreddit  \\\n",
       "0           NaN     58  psbattle_artwork   \n",
       "1           0.0     34     upliftingnews   \n",
       "2           NaN      8  psbattle_artwork   \n",
       "3           2.0      3       nottheonion   \n",
       "4           2.0     23       nottheonion   \n",
       "\n",
       "                                               title  upvote_ratio  \\\n",
       "0                                          Red Skull           NaN   \n",
       "1  Cafe in Bangkok With the Cutest Employees Ever...          0.78   \n",
       "2                                   He betrayed him!           NaN   \n",
       "3  Alderman Wants to Know Exactly What 'Bong Shop...          0.71   \n",
       "4  Man accused of stalking Scots police officer s...          0.84   \n",
       "\n",
       "   2_way_label  3_way_label  6_way_label       pos       neu       neg  \n",
       "0            0            2            4  0.250000  0.500000  0.250000  \n",
       "1            1            0            0  0.202655  0.357452  0.439893  \n",
       "2            0            2            4  0.250000  0.500000  0.250000  \n",
       "3            1            0            0  0.202655  0.357452  0.439893  \n",
       "4            1            0            0  0.202655  0.357452  0.439893  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cafe in bangkok with the cutest employees ever corgis'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid_train['clean_title'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidDataset(Dataset):\n",
    "    \"\"\"\n",
    "    torch dataset for Mid Model\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe) -> None:\n",
    "        super().__init__()\n",
    "        self.df = dataframe\n",
    "        self.ids = self.df['id'].values\n",
    "        self.labels = self.df['6_way_label'].values\n",
    "        self.information = self.df['clean_title'].values\n",
    "        self.imagepaths = self.df['id'].values\n",
    "        self.sent_scores = self.df[['pos', 'neu', 'neg']]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_id = self.ids[idx]\n",
    "        text = self.information[idx]\n",
    "        label = self.labels[idx]\n",
    "        imagepath = os.path.join(HOME, IMAGE_DATADIR, self.imagepaths[idx] + SUFFIX)\n",
    "        sentiment_scores = torch.tensor(self.sent_scores.iloc[idx].values)\n",
    "        return (item_id, text, imagepath, sentiment_scores), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on the text model features\n",
    "train_dataset = MidDataset(mid_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataset = MidDataset(mid_val)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataset = MidDataset(mid_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "        Text feature extractor (Bert Series)\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.text_model = AutoModel.from_pretrained(TEXT_MODEL_CKPT).to(DEVICE)\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_CKPT)\n",
    "\n",
    "    def forward(self, texts):\n",
    "        texts = [self._text_preprocessing(text) for text in texts]\n",
    "        encode_sent = self.text_tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LENGTH)\n",
    "        encode_sent['input_ids'] = torch.tensor(encode_sent['input_ids']).to(DEVICE)\n",
    "        encode_sent['attention_mask'] = torch.tensor(encode_sent['attention_mask']).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.text_model(**encode_sent)\n",
    "            last_hidden_state = outputs.last_hidden_state[:, 0]\n",
    "        return last_hidden_state\n",
    "\n",
    "    def _text_preprocessing(self, text):\n",
    "        \"\"\"\n",
    "        - Lowercase\n",
    "        - Remove entity name (e.g. @name)\n",
    "        @param text (str): a string to be processed\n",
    "        @return text (str): the processed string\n",
    "        \"\"\"\n",
    "        text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "        text = re.sub(r'&amp;', '&', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "class ViTImageFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Image feature extractor with ViT model\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.feature_extractor = ViTFeatureExtractor(IMAGE_MODEL_CKPT)\n",
    "        self.feature_model = ViTModel.from_pretrained(IMAGE_MODEL_CKPT).to(DEVICE)\n",
    "    def forward(self, imagefiles):\n",
    "        ims = [Image.open(imagefile) for imagefile in imagefiles]\n",
    "        ims = list(map(self._mode_convert, ims))\n",
    "        im_trans = self.feature_extractor(ims, return_tensors='pt').to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            features = self.feature_model(**im_trans)\n",
    "            last_hidden_state = features.last_hidden_state[:,0]\n",
    "        return last_hidden_state\n",
    "\n",
    "    def _mode_convert(self, im):\n",
    "        if im.mode != 'RGB':\n",
    "            im = im.convert(mode=\"RGB\")\n",
    "        return im\n",
    "\n",
    "\n",
    "class ImageFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Image feature extractor with ResNet50 / VGG16\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_model = models.resnet50(pretrained=True)\n",
    "        self.feature_model.fc = nn.Linear(2048, 768)\n",
    "        self.feature_model = self.feature_model.to(DEVICE)\n",
    "        self.training = True\n",
    "        \n",
    "    def forward(self, imagefiles):\n",
    "        ims = [Image.open(imagefile) for imagefile in imagefiles]\n",
    "        im_trans = torch.stack(list(map(self._im_transform, ims))).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            features = self.feature_model(im_trans)\n",
    "        return features\n",
    "\n",
    "    def _im_transform(self, im, train=True):\n",
    "        train = self.training\n",
    "        if im.mode != \"RGB\":\n",
    "            im = im.convert(mode=\"RGB\")\n",
    "        # transform the train data\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        # transform the test and validate data\n",
    "        transform_val = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        im_trans = transform_train(im) if train else transform_val(im)\n",
    "\n",
    "        return im_trans\n",
    "\n",
    "\n",
    "# class MultiFeatureClassifier(nn.Module):\n",
    "#     \"\"\"\n",
    "#     LSTM\n",
    "#     \"\"\"\n",
    "#     def __init__(self, image_model='vit') -> None:\n",
    "#         super().__init__()\n",
    "#         # common ANN for classifier\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(1539, 768),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(768, 6)\n",
    "#         )\n",
    "\n",
    "#         # basic LSTM for classification\n",
    "#         self.lstm = nn.LSTM(1539, 768, bidirectional=True, batch_first =True)\n",
    "#         self.fc1 = nn.Linear(1536, 768)\n",
    "#         self.bn = nn.BatchNorm1d(512)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(512, 6)\n",
    "\n",
    "\n",
    "#     def forward(self, term):\n",
    "#         outputs, _ = self.lstm(term)\n",
    "#         outputs = self.fc1(outputs)\n",
    "#         outputs = self.bn(outputs)\n",
    "#         outputs = self.relu(outputs)\n",
    "#         logits = self.fc2(outputs)\n",
    "#         return logits\n",
    "\n",
    "\n",
    "class MultiFeatureClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM + attention\n",
    "    \"\"\"\n",
    "    def __init__(self, image_model='vit') -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = {'vit': 1539, 'resnet': 1539, 'vgg': 1539}\n",
    "        # self.input_size = {'vit': 771, 'resnet': 771, 'vgg': 771}\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.input_size[image_model], 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 6)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(self.input_size[image_model], 768, 2)\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        self.bn = nn.BatchNorm1d(512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 6)\n",
    "\n",
    "        # When using AVG, ADD, MAXIMUM\n",
    "        # self.lstm = nn.LSTM(self.input_size[image_model], 512, 2)\n",
    "        # self.fc1 = nn.Linear(512, 256)\n",
    "        # self.bn = nn.BatchNorm1d(256)\n",
    "        # self.relu = nn.ReLU()\n",
    "        # self.fc2 = nn.Linear(256, 6)\n",
    "\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        lstm_output = lstm_output.permute(1, 0, 2)\n",
    "        merged_state = torch.mean(torch.cat([s for s in final_state]), 0)\n",
    "        hidden = merged_state.unsqueeze(1).unsqueeze(0)\n",
    "        attn_weights = torch.bmm(lstm_output, hidden)\n",
    "        soft_attn_weights = F.softmax(attn_weights, dim=1)\n",
    "\n",
    "        new_hidden_state = (lstm_output.transpose(1,2).squeeze() * soft_attn_weights.squeeze()).transpose(0,1)\n",
    "        return new_hidden_state\n",
    "\n",
    "    def forward(self, term):\n",
    "        # logits = self.classifier(term)\n",
    "        term = term.unsqueeze(1)\n",
    "        outputs, (hidden, cell) = self.lstm(term)\n",
    "        attn_output = self.attention_net(outputs, hidden)\n",
    "        attn_output = self.fc1(attn_output)\n",
    "        attn_output = self.bn(attn_output)\n",
    "        attn_output = self.relu(attn_output)\n",
    "        logits = self.fc2(attn_output)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class MidModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined Misinformation detection model\n",
    "\n",
    "    - text_feature_extractor\n",
    "    - image_feature_extractor\n",
    "    - final_feature_classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, image_model='vit'):\n",
    "        super().__init__()\n",
    "        self.image_model = image_model\n",
    "        self.training = True\n",
    "        self.text_feature_extractor = TextFeatureExtractor()\n",
    "        self.image_feature_extractor = ViTImageFeatureExtractor() if image_model == 'vit' else ImageFeatureExtractor()\n",
    "        self.feature_classifier = MultiFeatureClassifier(image_model).to(DEVICE)\n",
    "\n",
    "    def forward(self, item):\n",
    "        ids, texts, imagepaths, scores = item\n",
    "        text_features = self._textfeature(texts)\n",
    "        image_features = self._imagefeature(imagepaths)\n",
    "        fusion_features = self._combinefeature(text_features, image_features, sentiment_features=scores)\n",
    "        outputs = self.feature_classifier(fusion_features)\n",
    "        return outputs\n",
    "\n",
    "    def _textfeature(self, texts):\n",
    "        text_features = self.text_feature_extractor(texts)\n",
    "        return text_features\n",
    "\n",
    "    def _imagefeature(self, imagepaths):\n",
    "        if self.image_model != 'vit':\n",
    "            self.image_feature_extractor.training = self.training\n",
    "        image_features = self.image_feature_extractor(imagepaths)\n",
    "        return image_features\n",
    "\n",
    "    def _combinefeature(self, text_feature, image_feature, sentiment_features=None):\n",
    "        # feature fusion function\n",
    "        if sentiment_features != None:\n",
    "            sentiment_features = sentiment_features.float().to(DEVICE)\n",
    "        \n",
    "\n",
    "        fusion_features = {\"avg\": torch.mean(torch.stack((text_feature, image_feature), axis=1), 1), \n",
    "        \"concat\": torch.cat((text_feature, image_feature), axis=1) if sentiment_features == None else torch.cat((text_feature, image_feature, sentiment_features), axis=1), \n",
    "        \"max\": torch.max(torch.stack((text_feature, image_feature), axis=1), 1)[0], \n",
    "        \"add\": torch.add(text_feature, image_feature)}\n",
    "        fusion_type = 'concat'\n",
    "        fusion_features = fusion_features[fusion_type]\n",
    "        return fusion_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test final classifier\n",
    "# classifier = MultiFeatureClassifier()\n",
    "# a = torch.randn(64, 771)\n",
    "# classifier(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test image feature extractor\n",
    "# imagefeature = ImageFeatureExtractor()\n",
    "\n",
    "# for i in train_loader:\n",
    "#     item, labels = i\n",
    "#     ids, texts, imagepaths, scores = item\n",
    "#     features = imagefeature(imagepaths)\n",
    "#     print(features)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test feature extractors\n",
    "# text_feature_extractor = TextFeatureExtractor()\n",
    "# image_feature_extractor = ImageFeatureExtractor().to(DEVICE)\n",
    "\n",
    "# all_features = []\n",
    "# for item in tqdm(train_loader, total=len(train_loader)):\n",
    "#     ids, texts, imagepaths, scores = item\n",
    "#     image_features = image_feature_extractor(imagepaths)\n",
    "#     text_features = text_feature_extractor(texts)\n",
    "#     image_features = image_features.to('cpu')\n",
    "#     text_features = text_features.to('cpu')\n",
    "#     all_features.extend(list(zip(ids, text_features, image_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation metrics for model\n",
    "from sklearn.metrics import accuracy_score, f1_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return acc, f1\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model training and evaluation\n",
    "def train_model(model, trainloader, validloader, criterion, optimizer, scheduler, num_epochs = 5, valid=True):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    results = []\n",
    "    image_model = model.image_model\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "\n",
    "        pbar = tqdm(train_loader, total=len(trainloader))\n",
    "        for items in pbar:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            item, labels = items\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(item)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            pbar.set_postfix({'loss':loss})\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if valid == True:\n",
    "            result = validate_model(model, criterion, validloader, image_model)\n",
    "            results.append(result)\n",
    "            print(result)\n",
    "    return results\n",
    "\n",
    "def validate_model(model, critierion, valid_loader, image_model='vit'):\n",
    "    model.eval()\n",
    "\n",
    "    val_accuracy = []\n",
    "    val_f1 = []\n",
    "    val_loss = []\n",
    "\n",
    "    for items in tqdm(valid_loader, total=len(valid_loader)):\n",
    "        model.eval()\n",
    "        item, labels = items\n",
    "        with torch.no_grad():\n",
    "            if image_model != 'vit':\n",
    "                model.training = False\n",
    "                outputs = model(item)\n",
    "            else:\n",
    "                outputs = model(item)\n",
    "        labels = labels.to(\"cuda\")\n",
    "        loss = critierion(outputs, labels)\n",
    "        val_loss.append(loss.item())\n",
    "        preds = torch.argmax(outputs, 1).to(\"cpu\")\n",
    "        accuracy = (preds == labels.to('cpu')).numpy().mean() * 100\n",
    "        accuracy, f1_score = compute_metrics(preds, labels.to('cpu'))\n",
    "        val_accuracy.append(accuracy)\n",
    "        val_f1.append(f1_score)\n",
    "        \n",
    "    \n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "    val_f1 = np.mean(val_f1)\n",
    "    return val_loss, val_accuracy, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 128/1706 [01:17<15:35,  1.69it/s, loss=tensor(1.1948, device='cuda:0', grad_fn=<NllLossBackward0>)]/home/yangu/miniconda3/envs/th_env/lib/python3.10/site-packages/PIL/Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 95%|█████████▌| 1629/1706 [16:16<00:44,  1.74it/s, loss=tensor(0.9549, device='cuda:0', grad_fn=<NllLossBackward0>)]/home/yangu/miniconda3/envs/th_env/lib/python3.10/site-packages/PIL/Image.py:2896: DecompressionBombWarning: Image size (110718270 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1706/1706 [17:03<00:00,  1.67it/s, loss=tensor(0.5773, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
      "100%|██████████| 214/214 [02:19<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9924156036332389, 0.6797118380062305, 0.6548185738198155)\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 47/1706 [00:27<15:30,  1.78it/s, loss=tensor(0.5981, device='cuda:0', grad_fn=<NllLossBackward0>)]/home/yangu/miniconda3/envs/th_env/lib/python3.10/site-packages/PIL/Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 53%|█████▎    | 905/1706 [08:57<08:28,  1.58it/s, loss=tensor(0.6382, device='cuda:0', grad_fn=<NllLossBackward0>)]/home/yangu/miniconda3/envs/th_env/lib/python3.10/site-packages/PIL/Image.py:2896: DecompressionBombWarning: Image size (110718270 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1706/1706 [16:47<00:00,  1.69it/s, loss=tensor(0.8282, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
      "100%|██████████| 214/214 [02:19<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.2107341776941425, 0.5878602024922118, 0.5637056818990662)\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 96/1706 [00:56<15:18,  1.75it/s, loss=tensor(0.7438, device='cuda:0', grad_fn=<NllLossBackward0>)]/home/yangu/miniconda3/envs/th_env/lib/python3.10/site-packages/PIL/Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 45%|████▌     | 774/1706 [07:35<07:46,  2.00it/s, loss=tensor(0.7590, device='cuda:0', grad_fn=<NllLossBackward0>)]/home/yangu/miniconda3/envs/th_env/lib/python3.10/site-packages/PIL/Image.py:2896: DecompressionBombWarning: Image size (110718270 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1706/1706 [16:46<00:00,  1.69it/s, loss=tensor(0.5964, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
      "100%|██████████| 214/214 [02:19<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9346453021062868, 0.6731649143302182, 0.660376567391987)\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 23/1706 [00:14<16:16,  1.72it/s, loss=tensor(0.4999, device='cuda:0', grad_fn=<NllLossBackward0>)]/home/yangu/miniconda3/envs/th_env/lib/python3.10/site-packages/PIL/Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 27%|██▋       | 459/1706 [04:30<11:17,  1.84it/s, loss=tensor(0.5726, device='cuda:0', grad_fn=<NllLossBackward0>)]/home/yangu/miniconda3/envs/th_env/lib/python3.10/site-packages/PIL/Image.py:2896: DecompressionBombWarning: Image size (110718270 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1706/1706 [16:46<00:00,  1.69it/s, loss=tensor(0.4278, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
      "100%|██████████| 214/214 [02:19<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8002961836407118, 0.6707554517133957, 0.6492942095045882)\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 87/1706 [00:49<15:41,  1.72it/s, loss=tensor(0.5913, device='cuda:0', grad_fn=<NllLossBackward0>)]/home/yangu/miniconda3/envs/th_env/lib/python3.10/site-packages/PIL/Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "  8%|▊         | 139/1706 [01:22<19:15,  1.36it/s, loss=tensor(0.4646, device='cuda:0', grad_fn=<NllLossBackward0>)]/home/yangu/miniconda3/envs/th_env/lib/python3.10/site-packages/PIL/Image.py:2896: DecompressionBombWarning: Image size (110718270 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1706/1706 [16:47<00:00,  1.69it/s, loss=tensor(1.1549, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
      "100%|██████████| 214/214 [02:19<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7347763374308559, 0.7255646417445482, 0.7262114126948632)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mid_model = MidModel(image_model='vit')\n",
    "critierion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mid_model.feature_classifier.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "results = train_model(mid_model, train_loader, valid_loader, critierion, optimizer, exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [02:22<00:00,  1.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7728862527096383, 0.7193584501557632, 0.7203197505939838)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_model(mid_model, critierion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mid_model, 'roberta_vit_lsmt_attn_concat.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('th_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9195b6298d6a7344dfdd9b7dd22369761981621f82b452f68b5e000f5e42045d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
