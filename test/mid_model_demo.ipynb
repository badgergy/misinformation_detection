{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misinformation detection model (MID) full structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer, ViTFeatureExtractor, ViTModel, ViTForImageClassification\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.path.expanduser('~')\n",
    "TEXT_DATADIR = '~/Projects/Datasets/public_news_set'\n",
    "IMAGE_DATADIR = 'Projects/Datasets/public_image_set'\n",
    "# TRAIN_FILE = \"train_1000.tsv\"\n",
    "# TEST_FILE = \"test_100.tsv\"\n",
    "# VALID_FLIE = \"valid_100.tsv\"\n",
    "TRAIN_FILE = \"new_train_with_sentiment.tsv\"\n",
    "TEST_FILE = \"new_test_with_sentiment.tsv\"\n",
    "VALID_FLIE = \"new_valid_with_sentiment.tsv\"\n",
    "\n",
    "SUFFIX = '.jpg'\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TEXT_MODEL_CKPT = \"distilbert-base-uncased\"\n",
    "IMAGE_MODEL_CKPT = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "MAX_LENGTH = 32\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_train = pd.read_csv(os.path.join(TEXT_DATADIR, TRAIN_FILE), sep='\\t')\n",
    "mid_test = pd.read_csv(os.path.join(TEXT_DATADIR, TEST_FILE), sep='\\t')\n",
    "mid_val = pd.read_csv(os.path.join(TEXT_DATADIR, VALID_FLIE), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>domain</th>\n",
       "      <th>hasImage</th>\n",
       "      <th>id</th>\n",
       "      <th>image_url</th>\n",
       "      <th>linked_submission_id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>2_way_label</th>\n",
       "      <th>3_way_label</th>\n",
       "      <th>6_way_label</th>\n",
       "      <th>pos</th>\n",
       "      <th>neu</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jnoble50</td>\n",
       "      <td>red skull</td>\n",
       "      <td>1.553267e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>ej4e1lj</td>\n",
       "      <td>https://i.imgur.com/eD7QGRM.jpg</td>\n",
       "      <td>b44rhx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>psbattle_artwork</td>\n",
       "      <td>Red Skull</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gtash</td>\n",
       "      <td>cafe in bangkok with the cutest employees ever...</td>\n",
       "      <td>1.559911e+09</td>\n",
       "      <td>nynno.com</td>\n",
       "      <td>True</td>\n",
       "      <td>bxu2dd</td>\n",
       "      <td>https://external-preview.redd.it/MS7vkNibB3Yq1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34</td>\n",
       "      <td>upliftingnews</td>\n",
       "      <td>Cafe in Bangkok With the Cutest Employees Ever...</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202655</td>\n",
       "      <td>0.357452</td>\n",
       "      <td>0.439893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RoyalPrinceSoldier</td>\n",
       "      <td>he betrayed him</td>\n",
       "      <td>1.400820e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>chp14h4</td>\n",
       "      <td>http://i.imgur.com/9Q9CCDn.jpg</td>\n",
       "      <td>269qyi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>psbattle_artwork</td>\n",
       "      <td>He betrayed him!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>penguinseed</td>\n",
       "      <td>alderman wants to know exactly what bong shops...</td>\n",
       "      <td>1.403114e+09</td>\n",
       "      <td>dnainfo.com</td>\n",
       "      <td>True</td>\n",
       "      <td>28h8p1</td>\n",
       "      <td>https://external-preview.redd.it/lwbRUIzyGF5sU...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>nottheonion</td>\n",
       "      <td>Alderman Wants to Know Exactly What 'Bong Shop...</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202655</td>\n",
       "      <td>0.357452</td>\n",
       "      <td>0.439893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DM90</td>\n",
       "      <td>man accused of stalking scots police officer s...</td>\n",
       "      <td>1.383750e+09</td>\n",
       "      <td>dailyrecord.co.uk</td>\n",
       "      <td>True</td>\n",
       "      <td>1q10us</td>\n",
       "      <td>https://external-preview.redd.it/_fNXvGtKcKn_U...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23</td>\n",
       "      <td>nottheonion</td>\n",
       "      <td>Man accused of stalking Scots police officer s...</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202655</td>\n",
       "      <td>0.357452</td>\n",
       "      <td>0.439893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                        clean_title  \\\n",
       "0            jnoble50                                          red skull   \n",
       "1               Gtash  cafe in bangkok with the cutest employees ever...   \n",
       "2  RoyalPrinceSoldier                                    he betrayed him   \n",
       "3         penguinseed  alderman wants to know exactly what bong shops...   \n",
       "4                DM90  man accused of stalking scots police officer s...   \n",
       "\n",
       "    created_utc             domain  hasImage       id  \\\n",
       "0  1.553267e+09                NaN      True  ej4e1lj   \n",
       "1  1.559911e+09          nynno.com      True   bxu2dd   \n",
       "2  1.400820e+09                NaN      True  chp14h4   \n",
       "3  1.403114e+09        dnainfo.com      True   28h8p1   \n",
       "4  1.383750e+09  dailyrecord.co.uk      True   1q10us   \n",
       "\n",
       "                                           image_url linked_submission_id  \\\n",
       "0                    https://i.imgur.com/eD7QGRM.jpg               b44rhx   \n",
       "1  https://external-preview.redd.it/MS7vkNibB3Yq1...                  NaN   \n",
       "2                     http://i.imgur.com/9Q9CCDn.jpg               269qyi   \n",
       "3  https://external-preview.redd.it/lwbRUIzyGF5sU...                  NaN   \n",
       "4  https://external-preview.redd.it/_fNXvGtKcKn_U...                  NaN   \n",
       "\n",
       "   num_comments  score         subreddit  \\\n",
       "0           NaN     58  psbattle_artwork   \n",
       "1           0.0     34     upliftingnews   \n",
       "2           NaN      8  psbattle_artwork   \n",
       "3           2.0      3       nottheonion   \n",
       "4           2.0     23       nottheonion   \n",
       "\n",
       "                                               title  upvote_ratio  \\\n",
       "0                                          Red Skull           NaN   \n",
       "1  Cafe in Bangkok With the Cutest Employees Ever...          0.78   \n",
       "2                                   He betrayed him!           NaN   \n",
       "3  Alderman Wants to Know Exactly What 'Bong Shop...          0.71   \n",
       "4  Man accused of stalking Scots police officer s...          0.84   \n",
       "\n",
       "   2_way_label  3_way_label  6_way_label       pos       neu       neg  \n",
       "0            0            2            4  0.250000  0.500000  0.250000  \n",
       "1            1            0            0  0.202655  0.357452  0.439893  \n",
       "2            0            2            4  0.250000  0.500000  0.250000  \n",
       "3            1            0            0  0.202655  0.357452  0.439893  \n",
       "4            1            0            0  0.202655  0.357452  0.439893  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cafe in bangkok with the cutest employees ever corgis'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid_train['clean_title'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidDataset(Dataset):\n",
    "    \"\"\"\n",
    "    torch dataset for Mid Model\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe) -> None:\n",
    "        super().__init__()\n",
    "        self.df = dataframe\n",
    "        self.ids = self.df['id'].values\n",
    "        self.labels = self.df['6_way_label'].values\n",
    "        self.information = self.df['clean_title'].values\n",
    "        self.imagepaths = self.df['id'].values\n",
    "        self.sent_scores = self.df[['pos', 'neu', 'neg']]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_id = self.ids[idx]\n",
    "        text = self.information[idx]\n",
    "        label = self.labels[idx]\n",
    "        imagepath = os.path.join(HOME, IMAGE_DATADIR, self.imagepaths[idx] + SUFFIX)\n",
    "        sentiment_scores = torch.tensor(self.sent_scores.iloc[idx].values)\n",
    "        return (item_id, text, imagepath, sentiment_scores), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on the text model features\n",
    "train_dataset = MidDataset(mid_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "valid_dataset = MidDataset(mid_val)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataset = MidDataset(mid_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "        Text feature extractor\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.text_model = AutoModel.from_pretrained(TEXT_MODEL_CKPT).to(DEVICE)\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_CKPT)\n",
    "\n",
    "    def forward(self, texts):\n",
    "        texts = [self._text_preprocessing(text) for text in texts]\n",
    "        encode_sent = self.text_tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LENGTH)\n",
    "        encode_sent['input_ids'] = torch.tensor(encode_sent['input_ids']).to(DEVICE)\n",
    "        encode_sent['attention_mask'] = torch.tensor(encode_sent['attention_mask']).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.text_model(**encode_sent)\n",
    "            last_hidden_state = outputs.last_hidden_state[:, 0]\n",
    "        return last_hidden_state\n",
    "\n",
    "    def _text_preprocessing(self, text):\n",
    "        \"\"\"\n",
    "        - Lowercase\n",
    "        - Remove entity name (e.g. @name)\n",
    "        @param text (str): a string to be processed\n",
    "        @return text (str): the processed string\n",
    "        \"\"\"\n",
    "        text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "        text = re.sub(r'&amp;', '&', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "class ImageFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Image feature extractor\n",
    "    \"\"\"\n",
    "    # ViT model is temporarily selected as a feature extractor\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.feature_extractor = ViTFeatureExtractor(IMAGE_MODEL_CKPT)\n",
    "        self.feature_model = ViTModel.from_pretrained(IMAGE_MODEL_CKPT).to(DEVICE)\n",
    "    def forward(self, imagefiles):\n",
    "        ims = [Image.open(imagefile) for imagefile in imagefiles]\n",
    "        ims = list(map(self._mode_convert, ims))\n",
    "        im_trans = self.feature_extractor(ims, return_tensors='pt').to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            features = self.feature_model(**im_trans)\n",
    "            last_hidden_state = features.last_hidden_state[:,0]\n",
    "        return last_hidden_state\n",
    "\n",
    "    def _mode_convert(self, im):\n",
    "        if im.mode != 'RGB':\n",
    "            im = im.convert(mode=\"RGB\")\n",
    "        return im\n",
    "\n",
    "\n",
    "class MultiFeatureClassifier(nn.Module):\n",
    "    # TODO: change the classifier to LSTM or transformer\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # self.classifier = nn.Sequential(\n",
    "        #     nn.Linear(1536, 768),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(768, 512),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(512, 6)\n",
    "        # )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1536, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, term):\n",
    "        logits = self.classifier(term)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class MidModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text_feature_extractor = TextFeatureExtractor()\n",
    "        self.image_feature_extractor = ImageFeatureExtractor()\n",
    "        self.feature_classifier = MultiFeatureClassifier().to(DEVICE)\n",
    "\n",
    "    def forward(self, item):\n",
    "        ids, texts, imagepaths, scores = item\n",
    "        text_features = self._textfeature(texts)\n",
    "        image_features = self._imagefeature(imagepaths)\n",
    "        fusion_features = self._combinefeature(text_features, image_features)\n",
    "        outputs = self.feature_classifier(fusion_features)\n",
    "        return outputs\n",
    "\n",
    "    def _textfeature(self, texts):\n",
    "        text_features = self.text_feature_extractor(texts)\n",
    "        return text_features\n",
    "\n",
    "    def _imagefeature(self, imagepaths):\n",
    "\n",
    "        image_features = self.image_feature_extractor(imagepaths)\n",
    "        return image_features\n",
    "\n",
    "    def _combinefeature(self, text_feature, image_feature, sentiment_features=None):\n",
    "        # TODO: combine text image sentiment feature\n",
    "        fusion_features = torch.cat((text_feature, image_feature), axis=1)\n",
    "        return fusion_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1826, -0.3802,  0.1636, -0.1328, -0.1821, -0.1924],\n",
       "        [-0.1285, -0.4832,  0.3914, -0.2239, -0.0998,  0.0439],\n",
       "        [-0.1852, -0.4853, -0.0740,  0.1178, -0.3811, -0.1920]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultiFeatureClassifier()\n",
    "a = torch.randn(3, 1536)\n",
    "classifier(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_feature_extractor = TextFeatureExtractor()\n",
    "# image_feature_extractor = ImageFeatureExtractor().to(DEVICE)\n",
    "\n",
    "# all_features = []\n",
    "# for item in tqdm(train_loader, total=len(train_loader)):\n",
    "#     ids, texts, imagepaths, scores = item\n",
    "#     image_features = image_feature_extractor(imagepaths)\n",
    "#     text_features = text_feature_extractor(texts)\n",
    "#     image_features = image_features.to('cpu')\n",
    "#     text_features = text_features.to('cpu')\n",
    "#     all_features.extend(list(zip(ids, text_features, image_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cat((text_features, image_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## Train model\n",
    "mid_model = MidModel()\n",
    "\n",
    "critieron = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mid_model.feature_classifier.parameters(), lr=0.02)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "def train_model(model, trainloader, validloader, criterion, optimizer, scheduler, num_epochs = 25, valid=True):\n",
    "    model.train()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "\n",
    "        pbar = tqdm(train_loader, total=len(trainloader))\n",
    "        for items in pbar:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            item, labels = items\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(item)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            pbar.set_postfix({'loss':loss})\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if valid == True:\n",
    "            print(validate_model(model, validloader))\n",
    "\n",
    "def validate_model(model, valid_loader):\n",
    "    model.eval()\n",
    "\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    for items in tqdm(valid_loader, total=len(valid_loader)):\n",
    "        model.eval()\n",
    "        item, labels = items\n",
    "        with torch.no_grad():\n",
    "            outputs = model(item)\n",
    "        labels = labels.to(\"cuda\")\n",
    "        loss = critieron(outputs, labels)\n",
    "        val_loss.append(loss.item())\n",
    "        preds = torch.argmax(outputs, 1).to(\"cpu\")\n",
    "        accuracy = (preds == labels.to('cpu')).numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "    \n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 13/1706 [00:11<22:42,  1.24it/s, loss=tensor(1.3678, device='cuda:0', grad_fn=<NllLossBackward0>)]/home/yangu/miniconda3/envs/th_env/lib/python3.10/site-packages/PIL/Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 92%|█████████▏| 1567/1706 [21:46<01:47,  1.29it/s, loss=tensor(0.7913, device='cuda:0', grad_fn=<NllLossBackward0>)]/home/yangu/miniconda3/envs/th_env/lib/python3.10/site-packages/PIL/Image.py:2896: DecompressionBombWarning: Image size (110718270 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1706/1706 [23:43<00:00,  1.20it/s, loss=tensor(0.6960, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
      "100%|██████████| 214/214 [02:59<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5564075091453357, 80.05013629283488)\n",
      "Epoch 1/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1706/1706 [23:47<00:00,  1.19it/s, loss=tensor(0.5566, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
      "100%|██████████| 214/214 [02:56<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5558396768625652, 80.10611370716512)\n",
      "Epoch 2/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1706/1706 [23:21<00:00,  1.22it/s, loss=tensor(0.5311, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
      "100%|██████████| 214/214 [02:59<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5465067443307315, 80.3129867601246)\n",
      "Epoch 3/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 139/1706 [01:56<21:53,  1.19it/s, loss=tensor(0.4564, device='cuda:0', grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_model(mid_model, train_loader, valid_loader, critieron, optimizer, exp_lr_scheduler)\n",
      "\u001b[1;32m/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb Cell 17\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, trainloader, validloader, criterion, optimizer, scheduler, num_epochs, valid)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m item, labels \u001b[39m=\u001b[39m items\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(item)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(outputs, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/miniconda3/envs/th_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb Cell 17\u001b[0m in \u001b[0;36mMidModel.forward\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m ids, texts, imagepaths, scores \u001b[39m=\u001b[39m item\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m text_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_textfeature(texts)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m image_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_imagefeature(imagepaths)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m fusion_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combinefeature(text_features, image_features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_classifier(fusion_features)\n",
      "\u001b[1;32m/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb Cell 17\u001b[0m in \u001b[0;36mMidModel._imagefeature\u001b[0;34m(self, imagepaths)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_imagefeature\u001b[39m(\u001b[39mself\u001b[39m, imagepaths):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     image_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_feature_extractor(imagepaths)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m image_features\n",
      "File \u001b[0;32m~/miniconda3/envs/th_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb Cell 17\u001b[0m in \u001b[0;36mImageFeatureExtractor.forward\u001b[0;34m(self, imagefiles)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m ims \u001b[39m=\u001b[39m [Image\u001b[39m.\u001b[39mopen(imagefile) \u001b[39mfor\u001b[39;00m imagefile \u001b[39min\u001b[39;00m imagefiles]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m ims \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode_convert, ims))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m im_trans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_extractor(ims, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X22sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mim_trans)\n",
      "File \u001b[0;32m~/miniconda3/envs/th_env/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:141\u001b[0m, in \u001b[0;36mViTFeatureExtractor.__call__\u001b[0;34m(self, images, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39m# transformations (resizing + normalization)\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_resize \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize(image\u001b[39m=\u001b[39mimage, size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize, resample\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresample) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_normalize:\n\u001b[1;32m    143\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize(image\u001b[39m=\u001b[39mimage, mean\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_mean, std\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_std) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n",
      "File \u001b[0;32m~/miniconda3/envs/th_env/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:141\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39m# transformations (resizing + normalization)\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_resize \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresize(image\u001b[39m=\u001b[39;49mimage, size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, resample\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresample) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_normalize:\n\u001b[1;32m    143\u001b[0m     images \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize(image\u001b[39m=\u001b[39mimage, mean\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_mean, std\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_std) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n",
      "File \u001b[0;32m~/miniconda3/envs/th_env/lib/python3.10/site-packages/transformers/image_utils.py:248\u001b[0m, in \u001b[0;36mImageFeatureExtractionMixin.resize\u001b[0;34m(self, image, size, resample, default_to_square, max_size)\u001b[0m\n\u001b[1;32m    244\u001b[0m                 new_short, new_long \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(max_size \u001b[39m*\u001b[39m new_short \u001b[39m/\u001b[39m new_long), max_size\n\u001b[1;32m    246\u001b[0m         size \u001b[39m=\u001b[39m (new_short, new_long) \u001b[39mif\u001b[39;00m width \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m height \u001b[39melse\u001b[39;00m (new_long, new_short)\n\u001b[0;32m--> 248\u001b[0m \u001b[39mreturn\u001b[39;00m image\u001b[39m.\u001b[39;49mresize(size, resample\u001b[39m=\u001b[39;49mresample)\n",
      "File \u001b[0;32m~/miniconda3/envs/th_env/lib/python3.10/site-packages/PIL/Image.py:1961\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mresize(size, resample, box)\n\u001b[1;32m   1959\u001b[0m     \u001b[39mreturn\u001b[39;00m im\u001b[39m.\u001b[39mconvert(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode)\n\u001b[0;32m-> 1961\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1963\u001b[0m \u001b[39mif\u001b[39;00m reducing_gap \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m resample \u001b[39m!=\u001b[39m NEAREST:\n\u001b[1;32m   1964\u001b[0m     factor_x \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m((box[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m size[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m reducing_gap) \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/th_env/lib/python3.10/site-packages/PIL/ImageFile.py:253\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mimage file is truncated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(b)\u001b[39m}\u001b[39;00m\u001b[39m bytes not processed)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    252\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[0;32m--> 253\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[1;32m    254\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(mid_model, train_loader, valid_loader, critieron, optimizer, exp_lr_scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 214/214 [03:08<00:00,  1.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6007503149943931, 79.04497663551402)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_model(mid_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('th_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9195b6298d6a7344dfdd9b7dd22369761981621f82b452f68b5e000f5e42045d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
