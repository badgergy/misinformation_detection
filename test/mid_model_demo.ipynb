{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misinformation detection model (MID) full structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer, ViTFeatureExtractor, ViTModel, ViTForImageClassification\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seed(seed):\n",
    "    '''\n",
    "    Fixes random number generator seeds for reproducibility\n",
    "    '''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "same_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.path.expanduser('~')\n",
    "TEXT_DATADIR = '~/Projects/Datasets/public_news_set'\n",
    "IMAGE_DATADIR = 'Projects/Datasets/public_image_set'\n",
    "# TRAIN_FILE = \"train_1000.tsv\"\n",
    "# TEST_FILE = \"test_100.tsv\"\n",
    "# VALID_FLIE = \"valid_100.tsv\"\n",
    "TRAIN_FILE = \"new_train_with_sentiment.tsv\"\n",
    "TEST_FILE = \"new_test_with_sentiment.tsv\"\n",
    "VALID_FLIE = \"new_valid_with_sentiment.tsv\"\n",
    "\n",
    "SUFFIX = '.jpg'\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# TEXT_MODEL_CKPT = \"distilbert-base-uncased\"\n",
    "# TEXT_MODEL_CKPT = \"bert-base-uncased\"\n",
    "TEXT_MODEL_CKPT = \"roberta-base\"\n",
    "IMAGE_MODEL_CKPT = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "MAX_LENGTH = 32\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_train = pd.read_csv(os.path.join(TEXT_DATADIR, TRAIN_FILE), sep='\\t')\n",
    "mid_test = pd.read_csv(os.path.join(TEXT_DATADIR, TEST_FILE), sep='\\t')\n",
    "mid_val = pd.read_csv(os.path.join(TEXT_DATADIR, VALID_FLIE), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>clean_title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>domain</th>\n",
       "      <th>hasImage</th>\n",
       "      <th>id</th>\n",
       "      <th>image_url</th>\n",
       "      <th>linked_submission_id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>2_way_label</th>\n",
       "      <th>3_way_label</th>\n",
       "      <th>6_way_label</th>\n",
       "      <th>pos</th>\n",
       "      <th>neu</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jnoble50</td>\n",
       "      <td>red skull</td>\n",
       "      <td>1.553267e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>ej4e1lj</td>\n",
       "      <td>https://i.imgur.com/eD7QGRM.jpg</td>\n",
       "      <td>b44rhx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58</td>\n",
       "      <td>psbattle_artwork</td>\n",
       "      <td>Red Skull</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gtash</td>\n",
       "      <td>cafe in bangkok with the cutest employees ever...</td>\n",
       "      <td>1.559911e+09</td>\n",
       "      <td>nynno.com</td>\n",
       "      <td>True</td>\n",
       "      <td>bxu2dd</td>\n",
       "      <td>https://external-preview.redd.it/MS7vkNibB3Yq1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34</td>\n",
       "      <td>upliftingnews</td>\n",
       "      <td>Cafe in Bangkok With the Cutest Employees Ever...</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202655</td>\n",
       "      <td>0.357452</td>\n",
       "      <td>0.439893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RoyalPrinceSoldier</td>\n",
       "      <td>he betrayed him</td>\n",
       "      <td>1.400820e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>chp14h4</td>\n",
       "      <td>http://i.imgur.com/9Q9CCDn.jpg</td>\n",
       "      <td>269qyi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>psbattle_artwork</td>\n",
       "      <td>He betrayed him!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>penguinseed</td>\n",
       "      <td>alderman wants to know exactly what bong shops...</td>\n",
       "      <td>1.403114e+09</td>\n",
       "      <td>dnainfo.com</td>\n",
       "      <td>True</td>\n",
       "      <td>28h8p1</td>\n",
       "      <td>https://external-preview.redd.it/lwbRUIzyGF5sU...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>nottheonion</td>\n",
       "      <td>Alderman Wants to Know Exactly What 'Bong Shop...</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202655</td>\n",
       "      <td>0.357452</td>\n",
       "      <td>0.439893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DM90</td>\n",
       "      <td>man accused of stalking scots police officer s...</td>\n",
       "      <td>1.383750e+09</td>\n",
       "      <td>dailyrecord.co.uk</td>\n",
       "      <td>True</td>\n",
       "      <td>1q10us</td>\n",
       "      <td>https://external-preview.redd.it/_fNXvGtKcKn_U...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23</td>\n",
       "      <td>nottheonion</td>\n",
       "      <td>Man accused of stalking Scots police officer s...</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202655</td>\n",
       "      <td>0.357452</td>\n",
       "      <td>0.439893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                                        clean_title  \\\n",
       "0            jnoble50                                          red skull   \n",
       "1               Gtash  cafe in bangkok with the cutest employees ever...   \n",
       "2  RoyalPrinceSoldier                                    he betrayed him   \n",
       "3         penguinseed  alderman wants to know exactly what bong shops...   \n",
       "4                DM90  man accused of stalking scots police officer s...   \n",
       "\n",
       "    created_utc             domain  hasImage       id  \\\n",
       "0  1.553267e+09                NaN      True  ej4e1lj   \n",
       "1  1.559911e+09          nynno.com      True   bxu2dd   \n",
       "2  1.400820e+09                NaN      True  chp14h4   \n",
       "3  1.403114e+09        dnainfo.com      True   28h8p1   \n",
       "4  1.383750e+09  dailyrecord.co.uk      True   1q10us   \n",
       "\n",
       "                                           image_url linked_submission_id  \\\n",
       "0                    https://i.imgur.com/eD7QGRM.jpg               b44rhx   \n",
       "1  https://external-preview.redd.it/MS7vkNibB3Yq1...                  NaN   \n",
       "2                     http://i.imgur.com/9Q9CCDn.jpg               269qyi   \n",
       "3  https://external-preview.redd.it/lwbRUIzyGF5sU...                  NaN   \n",
       "4  https://external-preview.redd.it/_fNXvGtKcKn_U...                  NaN   \n",
       "\n",
       "   num_comments  score         subreddit  \\\n",
       "0           NaN     58  psbattle_artwork   \n",
       "1           0.0     34     upliftingnews   \n",
       "2           NaN      8  psbattle_artwork   \n",
       "3           2.0      3       nottheonion   \n",
       "4           2.0     23       nottheonion   \n",
       "\n",
       "                                               title  upvote_ratio  \\\n",
       "0                                          Red Skull           NaN   \n",
       "1  Cafe in Bangkok With the Cutest Employees Ever...          0.78   \n",
       "2                                   He betrayed him!           NaN   \n",
       "3  Alderman Wants to Know Exactly What 'Bong Shop...          0.71   \n",
       "4  Man accused of stalking Scots police officer s...          0.84   \n",
       "\n",
       "   2_way_label  3_way_label  6_way_label       pos       neu       neg  \n",
       "0            0            2            4  0.250000  0.500000  0.250000  \n",
       "1            1            0            0  0.202655  0.357452  0.439893  \n",
       "2            0            2            4  0.250000  0.500000  0.250000  \n",
       "3            1            0            0  0.202655  0.357452  0.439893  \n",
       "4            1            0            0  0.202655  0.357452  0.439893  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cafe in bangkok with the cutest employees ever corgis'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid_train['clean_title'].iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidDataset(Dataset):\n",
    "    \"\"\"\n",
    "    torch dataset for Mid Model\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe) -> None:\n",
    "        super().__init__()\n",
    "        self.df = dataframe\n",
    "        self.ids = self.df['id'].values\n",
    "        self.labels = self.df['6_way_label'].values\n",
    "        self.information = self.df['clean_title'].values\n",
    "        self.imagepaths = self.df['id'].values\n",
    "        self.sent_scores = self.df[['pos', 'neu', 'neg']]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_id = self.ids[idx]\n",
    "        text = self.information[idx]\n",
    "        label = self.labels[idx]\n",
    "        imagepath = os.path.join(HOME, IMAGE_DATADIR, self.imagepaths[idx] + SUFFIX)\n",
    "        sentiment_scores = torch.tensor(self.sent_scores.iloc[idx].values)\n",
    "        return (item_id, text, imagepath, sentiment_scores), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on the text model features\n",
    "train_dataset = MidDataset(mid_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataset = MidDataset(mid_val)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataset = MidDataset(mid_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "        Text feature extractor (Bert Series)\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.text_model = AutoModel.from_pretrained(TEXT_MODEL_CKPT).to(DEVICE)\n",
    "        self.text_tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_CKPT)\n",
    "\n",
    "    def forward(self, texts):\n",
    "        texts = [self._text_preprocessing(text) for text in texts]\n",
    "        encode_sent = self.text_tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LENGTH)\n",
    "        encode_sent['input_ids'] = torch.tensor(encode_sent['input_ids']).to(DEVICE)\n",
    "        encode_sent['attention_mask'] = torch.tensor(encode_sent['attention_mask']).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.text_model(**encode_sent)\n",
    "            last_hidden_state = outputs.last_hidden_state[:, 0]\n",
    "        return last_hidden_state\n",
    "\n",
    "    def _text_preprocessing(self, text):\n",
    "        \"\"\"\n",
    "        - Lowercase\n",
    "        - Remove entity name (e.g. @name)\n",
    "        @param text (str): a string to be processed\n",
    "        @return text (str): the processed string\n",
    "        \"\"\"\n",
    "        text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "        text = re.sub(r'&amp;', '&', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "class ViTImageFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Image feature extractor with ViT model\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.feature_extractor = ViTFeatureExtractor(IMAGE_MODEL_CKPT)\n",
    "        self.feature_model = ViTModel.from_pretrained(IMAGE_MODEL_CKPT).to(DEVICE)\n",
    "    def forward(self, imagefiles):\n",
    "        ims = [Image.open(imagefile) for imagefile in imagefiles]\n",
    "        ims = list(map(self._mode_convert, ims))\n",
    "        im_trans = self.feature_extractor(ims, return_tensors='pt').to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            features = self.feature_model(**im_trans)\n",
    "            last_hidden_state = features.last_hidden_state[:,0]\n",
    "        return last_hidden_state\n",
    "\n",
    "    def _mode_convert(self, im):\n",
    "        if im.mode != 'RGB':\n",
    "            im = im.convert(mode=\"RGB\")\n",
    "        return im\n",
    "\n",
    "\n",
    "class ImageFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Image feature extractor with ResNet50 / VGG16\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_model = models.resnet50(pretrained=True)\n",
    "        self.feature_model.fc = nn.Linear(2048, 768)\n",
    "        self.feature_model = self.feature_model.to(DEVICE)\n",
    "        self.training = True\n",
    "        \n",
    "    def forward(self, imagefiles):\n",
    "        ims = [Image.open(imagefile) for imagefile in imagefiles]\n",
    "        im_trans = torch.stack(list(map(self._im_transform, ims))).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            features = self.feature_model(im_trans)\n",
    "        return features\n",
    "\n",
    "    def _im_transform(self, im, train=True):\n",
    "        train = self.training\n",
    "        if im.mode != \"RGB\":\n",
    "            im = im.convert(mode=\"RGB\")\n",
    "        # transform the train data\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        # transform the test and validate data\n",
    "        transform_val = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        im_trans = transform_train(im) if train else transform_val(im)\n",
    "\n",
    "        return im_trans\n",
    "\n",
    "\n",
    "class MultiFeatureClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # common ANN for classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1539, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 6)\n",
    "        )\n",
    "\n",
    "        # basic LSTM for classification\n",
    "        self.lstm = nn.LSTM(1539, 768, bidirectional=True, batch_first =True)\n",
    "        self.fc1 = nn.Linear(1536, 768)\n",
    "        self.bn = nn.BatchNorm1d(512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 6)\n",
    "\n",
    "\n",
    "    def forward(self, term):\n",
    "        # outputs = self.transformer_encoder(term)\n",
    "        outputs, _ = self.lstm(term)\n",
    "        outputs = self.fc1(outputs)\n",
    "        outputs = self.bn(outputs)\n",
    "        outputs = self.relu(outputs)\n",
    "        logits = self.fc2(outputs)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class MultiFeatureClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM + attention\n",
    "    \"\"\"\n",
    "    def __init__(self, image_model='vit') -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = {'vit': 1539, 'resnet': 1539, 'vgg': 4867}\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.input_size[image_model], 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, 6)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(self.input_size[image_model], 768, 2)\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        self.bn = nn.BatchNorm1d(512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 6)\n",
    "\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        lstm_output = lstm_output.permute(1, 0, 2)\n",
    "        merged_state = torch.mean(torch.cat([s for s in final_state]), 0)\n",
    "        hidden = merged_state.unsqueeze(1).unsqueeze(0)\n",
    "        attn_weights = torch.bmm(lstm_output, hidden)\n",
    "        soft_attn_weights = F.softmax(attn_weights, dim=1)\n",
    "\n",
    "        new_hidden_state = (lstm_output.transpose(1,2).squeeze() * soft_attn_weights.squeeze()).transpose(0,1)\n",
    "        return new_hidden_state\n",
    "\n",
    "    def forward(self, term):\n",
    "        # term = term.unsqueeze(1)\n",
    "        # outputs, (hidden, cell) = self.lstm(term)\n",
    "        # attn_output = self.attention_net(outputs, hidden)\n",
    "        # attn_output = self.fc1(attn_output)\n",
    "        # attn_output = self.bn(attn_output)\n",
    "        # attn_output = self.relu(attn_output)\n",
    "        # logits = self.fc2(attn_output)\n",
    "        logits = self.classifier(term)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MidModel(nn.Module):\n",
    "    def __init__(self, image_model='vit'):\n",
    "        super().__init__()\n",
    "        self.image_model = image_model\n",
    "        self.training = True\n",
    "        self.text_feature_extractor = TextFeatureExtractor()\n",
    "        self.image_feature_extractor = ViTImageFeatureExtractor() if image_model == 'vit' else ImageFeatureExtractor()\n",
    "        self.feature_classifier = MultiFeatureClassifier(image_model).to(DEVICE)\n",
    "\n",
    "    def forward(self, item):\n",
    "        ids, texts, imagepaths, scores = item\n",
    "        text_features = self._textfeature(texts)\n",
    "        image_features = self._imagefeature(imagepaths)\n",
    "        fusion_features = self._combinefeature(text_features, image_features, sentiment_features=scores)\n",
    "        outputs = self.feature_classifier(fusion_features)\n",
    "        return outputs\n",
    "\n",
    "    def _textfeature(self, texts):\n",
    "        text_features = self.text_feature_extractor(texts)\n",
    "        return text_features\n",
    "\n",
    "    def _imagefeature(self, imagepaths):\n",
    "        if self.image_model != 'vit':\n",
    "            self.image_feature_extractor.training = self.training\n",
    "        image_features = self.image_feature_extractor(imagepaths)\n",
    "        return image_features\n",
    "\n",
    "    def _combinefeature(self, text_feature, image_feature, sentiment_features=None):\n",
    "        if sentiment_features != None:\n",
    "            sentiment_features = sentiment_features.float().to(DEVICE)\n",
    "        \n",
    "        fusion_features = torch.cat((text_feature, image_feature), axis=1) if sentiment_features == None else torch.cat((text_feature, sentiment_features, image_feature), axis=1)\n",
    "        return fusion_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test final classifier\n",
    "classifier = MultiFeatureClassifier()\n",
    "a = torch.randn(64, 1539)\n",
    "classifier(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1046,  0.4809,  0.2576,  ...,  0.3444,  0.3518,  0.2544],\n",
      "        [-0.2915,  0.4736,  0.0614,  ...,  0.0478,  0.6280, -0.3919],\n",
      "        [-0.3044,  0.3286,  0.2695,  ...,  0.0605,  0.4982, -0.0940],\n",
      "        ...,\n",
      "        [-0.2487,  0.1355,  0.0941,  ...,  0.5524,  0.5215,  0.4005],\n",
      "        [ 0.1414,  0.0319,  0.0988,  ..., -0.0450,  0.4359,  0.3888],\n",
      "        [-0.1296,  0.1772,  0.2801,  ..., -0.1226,  0.2787,  0.0326]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test image feature extractor\n",
    "imagefeature = ImageFeatureExtractor()\n",
    "\n",
    "for i in train_loader:\n",
    "    item, labels = i\n",
    "    ids, texts, imagepaths, scores = item\n",
    "    features = imagefeature(imagepaths)\n",
    "    print(features)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test feature extractors\n",
    "# text_feature_extractor = TextFeatureExtractor()\n",
    "# image_feature_extractor = ImageFeatureExtractor().to(DEVICE)\n",
    "\n",
    "# all_features = []\n",
    "# for item in tqdm(train_loader, total=len(train_loader)):\n",
    "#     ids, texts, imagepaths, scores = item\n",
    "#     image_features = image_feature_extractor(imagepaths)\n",
    "#     text_features = text_feature_extractor(texts)\n",
    "#     image_features = image_features.to('cpu')\n",
    "#     text_features = text_features.to('cpu')\n",
    "#     all_features.extend(list(zip(ids, text_features, image_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test feature fusion\n",
    "# torch.cat((text_features, image_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation metrics for model\n",
    "from sklearn.metrics import accuracy_score, f1_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model training and evaluation\n",
    "def train_model(model, trainloader, validloader, criterion, optimizer, scheduler, num_epochs = 5, valid=True):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    image_model = model.image_model\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "\n",
    "        pbar = tqdm(train_loader, total=len(trainloader))\n",
    "        for items in pbar:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            item, labels = items\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(item)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            pbar.set_postfix({'loss':loss})\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if valid == True:\n",
    "            print(validate_model(model, criterion, validloader, image_model))\n",
    "\n",
    "def validate_model(model, critierion, valid_loader, image_model='vit'):\n",
    "    model.eval()\n",
    "\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    for items in tqdm(valid_loader, total=len(valid_loader)):\n",
    "        model.eval()\n",
    "        item, labels = items\n",
    "        with torch.no_grad():\n",
    "            if image_model != 'vit':\n",
    "                model.training = False\n",
    "                outputs = model(item)\n",
    "            else:\n",
    "                outputs = model(item)\n",
    "        labels = labels.to(\"cuda\")\n",
    "        loss = critierion(outputs, labels)\n",
    "        val_loss.append(loss.item())\n",
    "        preds = torch.argmax(outputs, 1).to(\"cpu\")\n",
    "        accuracy = (preds == labels.to('cpu')).numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "    \n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 30/1706 [00:17<15:48,  1.77it/s, loss=tensor(1.2491, device='cuda:0', grad_fn=<NllLossBackward0>)]/home/yangu/miniconda3/envs/th_env/lib/python3.10/site-packages/PIL/Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      " 59%|█████▉    | 1013/1706 [10:10<06:35,  1.75it/s, loss=tensor(0.7397, device='cuda:0', grad_fn=<NllLossBackward0>)]/home/yangu/miniconda3/envs/th_env/lib/python3.10/site-packages/PIL/Image.py:2896: DecompressionBombWarning: Image size (110718270 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1706/1706 [17:14<00:00,  1.65it/s, loss=tensor(0.6256, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
      "100%|██████████| 214/214 [02:21<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7172805863284619, 73.48130841121495)\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1706/1706 [16:52<00:00,  1.68it/s, loss=tensor(0.8590, device='cuda:0', grad_fn=<NllLossBackward0>)]\n",
      "100%|██████████| 214/214 [02:21<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6317783666548328, 77.27803738317758)\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 108/1706 [01:06<16:26,  1.62it/s, loss=tensor(0.5746, device='cuda:0', grad_fn=<NllLossBackward0>)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(mid_model\u001b[39m.\u001b[39mfeature_classifier\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.0005\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m exp_lr_scheduler \u001b[39m=\u001b[39m lr_scheduler\u001b[39m.\u001b[39mStepLR(optimizer, step_size\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_model(mid_model, train_loader, valid_loader, critierion, optimizer, exp_lr_scheduler)\n",
      "\u001b[1;32m/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb Cell 19\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, trainloader, validloader, criterion, optimizer, scheduler, num_epochs, valid)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m item, labels \u001b[39m=\u001b[39m items\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(item)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(outputs, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/miniconda3/envs/th_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb Cell 19\u001b[0m in \u001b[0;36mMidModel.forward\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m ids, texts, imagepaths, scores \u001b[39m=\u001b[39m item\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m text_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_textfeature(texts)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=185'>186</a>\u001b[0m image_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_imagefeature(imagepaths)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m fusion_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combinefeature(text_features, image_features, sentiment_features\u001b[39m=\u001b[39mscores)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=187'>188</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_classifier(fusion_features)\n",
      "\u001b[1;32m/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb Cell 19\u001b[0m in \u001b[0;36mMidModel._imagefeature\u001b[0;34m(self, imagepaths)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=195'>196</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_model \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvit\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=196'>197</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_feature_extractor\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=197'>198</a>\u001b[0m image_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_feature_extractor(imagepaths)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=198'>199</a>\u001b[0m \u001b[39mreturn\u001b[39;00m image_features\n",
      "File \u001b[0;32m~/miniconda3/envs/th_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb Cell 19\u001b[0m in \u001b[0;36mImageFeatureExtractor.forward\u001b[0;34m(self, imagefiles)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, imagefiles):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     ims \u001b[39m=\u001b[39m [Image\u001b[39m.\u001b[39mopen(imagefile) \u001b[39mfor\u001b[39;00m imagefile \u001b[39min\u001b[39;00m imagefiles]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     im_trans \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(\u001b[39mlist\u001b[39;49m(\u001b[39mmap\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_im_transform, ims)))\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m         features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_model(im_trans)\n",
      "\u001b[1;32m/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb Cell 19\u001b[0m in \u001b[0;36mImageFeatureExtractor._im_transform\u001b[0;34m(self, im, train)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39m# transform the test and validate data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m transform_val \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     transforms\u001b[39m.\u001b[39mResize(\u001b[39m256\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m     transforms\u001b[39m.\u001b[39mCenterCrop(\u001b[39m224\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m     transforms\u001b[39m.\u001b[39mNormalize([\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m], [\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m ])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m im_trans \u001b[39m=\u001b[39m transform_train(im) \u001b[39mif\u001b[39;00m train \u001b[39melse\u001b[39;00m transform_val(im)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/yangu/Projects/Dissertation/test/mid_model_demo.ipynb#X23sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m \u001b[39mreturn\u001b[39;00m im_trans\n",
      "File \u001b[0;32m~/miniconda3/envs/th_env/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/envs/th_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/th_env/lib/python3.10/site-packages/torchvision/transforms/transforms.py:953\u001b[0m, in \u001b[0;36mRandomResizedCrop.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    947\u001b[0m \u001b[39m    img (PIL Image or Tensor): Image to be cropped and resized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[39m    PIL Image or Tensor: Randomly cropped and resized image.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    952\u001b[0m i, j, h, w \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params(img, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mratio)\n\u001b[0;32m--> 953\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresized_crop(img, i, j, h, w, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation)\n",
      "File \u001b[0;32m~/miniconda3/envs/th_env/lib/python3.10/site-packages/torchvision/transforms/functional.py:589\u001b[0m, in \u001b[0;36mresized_crop\u001b[0;34m(img, top, left, height, width, size, interpolation)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[1;32m    588\u001b[0m     _log_api_usage_once(resized_crop)\n\u001b[0;32m--> 589\u001b[0m img \u001b[39m=\u001b[39m crop(img, top, left, height, width)\n\u001b[1;32m    590\u001b[0m img \u001b[39m=\u001b[39m resize(img, size, interpolation)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/envs/th_env/lib/python3.10/site-packages/torchvision/transforms/functional.py:510\u001b[0m, in \u001b[0;36mcrop\u001b[0;34m(img, top, left, height, width)\u001b[0m\n\u001b[1;32m    508\u001b[0m     _log_api_usage_once(crop)\n\u001b[1;32m    509\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 510\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mcrop(img, top, left, height, width)\n\u001b[1;32m    512\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mcrop(img, top, left, height, width)\n",
      "File \u001b[0;32m~/miniconda3/envs/th_env/lib/python3.10/site-packages/torchvision/transforms/functional_pil.py:221\u001b[0m, in \u001b[0;36mcrop\u001b[0;34m(img, top, left, height, width)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_pil_image(img):\n\u001b[1;32m    219\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be PIL Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(img)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 221\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mcrop((left, top, left \u001b[39m+\u001b[39;49m width, top \u001b[39m+\u001b[39;49m height))\n",
      "File \u001b[0;32m~/miniconda3/envs/th_env/lib/python3.10/site-packages/PIL/Image.py:1146\u001b[0m, in \u001b[0;36mImage.crop\u001b[0;34m(self, box)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[39mif\u001b[39;00m box \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1144\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m-> 1146\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1147\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_crop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mim, box))\n",
      "File \u001b[0;32m~/miniconda3/envs/th_env/lib/python3.10/site-packages/PIL/ImageFile.py:253\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mimage file is truncated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(b)\u001b[39m}\u001b[39;00m\u001b[39m bytes not processed)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    252\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[0;32m--> 253\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[1;32m    254\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mid_model = MidModel(image_model='resnet')\n",
    "critierion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mid_model.feature_classifier.parameters(), lr=0.0005)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "train_model(mid_model, train_loader, valid_loader, critierion, optimizer, exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate_model(mid_model, critierion, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(mid_model, 'roberta_vit_lstm_attn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('th_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9195b6298d6a7344dfdd9b7dd22369761981621f82b452f68b5e000f5e42045d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
